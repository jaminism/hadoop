출처 : [빅데이터 전문가의 하둡 관리](https://book.naver.com/bookdb/book_detail.nhn?bid=14134329) (샘 R. 알라파티 저 | 안진섭 역 | 성안당)




# 빅데이터 플랫폼
**하둡에서 각 처리 단계를 담당하는 소프트웨어 컴포넌트를 모듈이라 칭함.**

### 1. 수집 모듈
    - 플럼
        클라우데라에서 개발. 
        서버에 Agent를 심어두고 이벤트 발생 시 데이터 전송

    - 카프카
        링크드인에서 개발.
        퍼블리셔와 컨슈머 형태로 동작하고 여러 대의 서버에 브로커 라는 프로세스를 띄워 프로듀서라는 프로세스들이 브로커에 데이터 전달.
    - 스쿱
        아파치에서 개발.  
        RDBMS와 연동이 목적. 최근에는 플럼 카프카도 RDBMS 연동 가능.
        
### 2. 데이터 저장 및 처리 모듈
    - 하둡
        HDFS, MapReduce 제공
    - HBase
        HDFS를 저장소로 사용하는 컬럼 기반 NoSQL DB
    - 카산드라
        페북에서 개발.
        HBase와 같은 컬럼 기반 NoSQL 데이터베이스로 분류
    - 레디스(Redis)
        메모리를 이용한 키, 값 형태의 저상소
    - 피그
        피그라틴이라는 스크립트 인어로 작성하여 맵리듀스 실행
    - 하이브
        SQL 과 비슷한 스크립트 언어를 제공하여 맵리듀스 실행
    - 스파크
        하둡과 유사한 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크.
        하둡에 비해 반복적인 데이터를 처리하는 데 뛰어난 성능을 보인다(인메모리 기반)
        
### 3. 데이터 분석 및 기타 소프트웨어 모듈
    - R
        데이터에 대한 수치 처리 및 그래픽 처리 기능을 제공.
    - 클라우데라, 호튼웍스
        CDH, HDP 으로 배포되는 하둡 에코시스템을 구성하는 플랫폼.
    - 엘라스틱 서치
        루씬을 기반으로 개발된 RESTful 분산 검색 엔진.
        주로 JSON 형태로 데이터를 저장하고 검색, 집계, 통계, 머신러닝 등의 영역이 활용.
    
    
### - 기타 정보
    암바리(Ambari : 클러스터 관리),
    아브로 (Avro; 데이터 직렬화).
    카산드라 (Cassandra; 멀티마스터 데이터베이스),
    척와 (Chukwa : 데이터 수집),
    에이치베이스 (HBase : 분산 데이터베이스)
    하이브(Hive ; 데이터 창고,
    머하웃(Mahout : ML 및 데이터 마이닝),
    피그 (Pig : 실행 프레임워크).
    스파크 (Spark : 검퓨트 엔진).
    테즈 (Tez : 맵리듀스를 대체할 데이터 플로우 프로그래밍 프레임워크).
    주키퍼 (ZooKeeper 코디네이션 서비스)
    아파치 스톰(Storm ; 스트림 처리),
    카프카 (Kafka , 메시지 전송).
    부가가치를 제공하는 아마존 EMR(Elastic Map Reduce),
    클리우데라 (Cloudera),
    호튼웍스 (Hortonworks),
    마이크로스프트 (에이치디인사이트(HDInsight).
    맵알 (MapR),
    SAP 알티스케일 (Altiscale) 도 있다.

    아파치 스쿱 = 관계형 데이터베이스의 데이터를 HDFS로 옮기는 컴포넌트
    아파치 들룸 = 많은 양의 스트리밍 데이터를 수집, 종합하고 다양한 소스로부터 HDFS와 같은 데이터 저장소로 이동하는 시스템.
    아파치 카프가 = 링크드인에서 개발, 병렬 로딩이 가능하고 실시간 데이터 소비를 클러스터로 분할해줌으로써 오프라인과 온라인 스트리밍 데이터 처리를 결합할 수 있다.
    이외에도 엘라스틱써치, Logstash, Kibana 있다.



## 버전별 특징 
### 1) version 1
-   병렬처리 (맵리듀스) 
잡트래커와 태스크트래커가 담당 
- 분산저장(HDFS) 
네임노드와 데이터노드가 담당
-   잡트래커가 클러스터 자원관리와 라이프사이클 관리를 모두 함에 병목현상 발생하여 문제가 됨

### 2) version 2
- 클러스터 관리 
리소스 매니저, 노드 매니저 
- 작업 관리 
애플리케이션 마스터, 컨테이너
- YARN 아키텍처의 작업처리 단위는 컨테이너.
- 프로세스
	작업 제출 > AM 생성 > AM이 RM에 지원 요청 > RM이 컨테이너를 AM 에 할당 > 컨테이너가 할당 되면 생성 되고 작업 완료 후 종료
### 3) version 3
- 이레이져 코딩 도입으로 HDFS 데이터 저장 효율성 증가

> 잡트레커 = RM+AM 이 역할 수행 
> 테스트크래커 = 노드매니저(워커노드 관리) 

 
 
## 다중 처리 엔진들
- 배치 프로세싱 - 맵리듀스, 하이브/피그, 테즈
-   인터랙티브 SQL - 테즈
-   온라인 데이터베이스 - HBase
-   스트리밍 - 스톰, 스파크, Sanza | 인메모리 - 스파크
-   그래프 프로세싱 - 지라프, 스파크 그러프엑스 . HPC MPI - OpenMPI
-   대용량 검색 - 솔라 


## HDFS
- HDFS (Hadoop Distributed File System) 는 범용 하드웨어에서 동작하고, 장애 복구성을 가지는 분산 파일 시스템을 목표. 
- 단일 실패 지점 (SPOF)이 되기 때문에 네임노드 관리가 중요 

- 특징  
 블록 단위 저장 블록 복제를 이용한 장애 복구 읽기 중심 . 
 데이터 지역성 (데이터가 있는곳에서 알고리즘 처리 > 네트워크 이동 비용 감소)

1.  구조 
	1-1.  네임노드
	- 메타데이터 관리 
		- 파일이름, 크기, 생성시간, 권한 등의 정보 
		- Fsimage(네임스페이스, 블록 정보) 
		- Edits (파일 관련 로그, 메모리 저장하다가 주기적 생성) 

### 잡 스케줄링 
- 우지 
	- 제어 의존성이 비순환 방향 그래프(DAG) 로 정렬되는 워크플로 스케줄링 시스템. 
	- 하둡의 잡은 대개 상호의존적이라 크론으로는 우지를 대체할 수 없다. 

### 하둡 데이터 보안
- 기본적으로 하둡은 롤 기반 액세스나 객체 수준의 액세스를 설정할 수 없다. 
- 그래서 인증받지 않은 사용자들이 노드 에 접근해 클러스터를 공격할 수도 있다. 
- 커버로스 
		- 사용자 인증 부분의 보안 관련 오픈소스 인증 메커니즘


## 하둡 필요조건
- 확장성 (Scalability)
- 장애허용 (Fault tolerance)
- 회복력 (Recoverability) | 
---
- HDFS 에서 파일생성이나 삭제가 발생하면 메타데이터가 변경되고 **fsImage** 에 기록이 반영된다. 그리고 매 시간(기본값)마다 보조 네임노드가 fsImage 파일을 합치는 작업을 한다.

-   클라이언트는 데이터노드에 직접 접근할 수 없어 어떤 데이터노드가 호스팅 하는지는 알수 없지만, 네임노드에 접속하여 파일 블록 Number나 데이터위치(데이터노드)를 알수 있다. 즉, 네임노드는 클라이언트가 데이터를 기록할 수 있는 데이터노드 목록을 보내주는 일을 함으로써 클라이언트가 어디에 기록할지 결정할 수 있다.
- 초기 메타데이터는 fsImage에 저장되며 파일의 생성, 삭제 등으로 변화할 때 트랜잭션 로그가 기록된다. 세컨더리 네임노드는 주기적으로 fsImage와 로그파일을 통합한다. 이 작업이 완료되면 네임노드는 변경내용을 기록한 파일을 삭제한다. 
- 얀은 분산 애플리케이션을 관리하기 위한 관리 프레임워크. 얀은 하둡 프로세싱 레이어로 리소스 매니저와 잡 스케줄러를 포함한다. 이는 다른 분산 프로세싱 프레임워크인 맵리듀 스, 스파크, 테즈를 포함한 하둡 환경에서 작동한다. 
- 얀은 컨테이너를 사용해 애플리케이션을 실행한다. 리소스매니저는 컨테이너를 각 애플리게이션에 스케줄링하고, 노드매니저는 컨테이너의 라이프사이클을 관리한다.  
- 얀은 다양한 실행 엔진들로 동작하는 다양한 환경을 관리함 -> 배치 프로세싱, 인터랙티브 환경, 실시간 프로세싱을 위한 엔진이 동작하면 HDFS에 접근 가능 
- 얀은 어플리케이션 작업량을 관리하고 모니터링 함, 맵, 리듀스 태스크가 완료되면 컨테이너는 소멸되고 남은 다른 태스크가 있을 때 새로운 컨테이너가 생성된다. 280 AS , 리소스매니저는 클러스터 당 하나만 존재한다. 역할은 다음과 같다.
-   모든 얀 애플리케이션의 시작을 위한 처리
-   잡 스케줄링과 실행 관리
-   모든 데이터노드의 리소스 할당
-   데이터노드를 관리하기 위해 노드매니저의 하트비트 관리 노드매니저의 기능
-   하드비트와 컨테이너 상태 알림을 통해 리소스매니저와 정보를 주고 받는다.
-   애플리케이션 컨테이너의 라이프 사이클을 관리
-   데이터노드의 상태 추적
-   컨데이너 리소스 사용량 정보를 관리 애플리게이션 마스터 각각의 애플리케이션에는 전용 애플리케이션 마스터가 존재
-   태스크 스케줄링과 실행 관리
-   애플리케이션 태스크를 위해 로컬 리소스 할당
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTI1NzgzNzMyMiwtMTMyNzc0MDMxNywtMT
QzNzczNjYxMywtMTk5MTk4MzYxNF19
-->